---
title: "Chap9 - SVM"
author: "Me"
date: "8/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Support vector machines

In this lab, we'll use `e1071` library to demonstrate the support vector classifier and the SVM. Another option is `Liblinear` library, which is particulary useful for very large linear problem

# Support vector classifier

The `e1071` library contains implementations for a number of statistical learning methods. In particular, the `svm()` function can be used to fit a support vector classifier when the argument `kernel = linear` is used. This function uses a slightly different formulation to the equations we saw in lecture to build the support vector classifier. A `cost` argumetn allows us to specify cost of a violation to the margin. When the `cost` is small, then the margins will be wide and many support vectors will be on the margin or violate the margin. When the `cost` argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin

We can use `svm()` function to fit the support vector classifier for a given value of `cost` parameter. Here we demonstrate the use of this function on a two dimensional example so we can plot the resulting decision boundary. Let's start by generating set of observartions, which belong to two classes




```{r}
set.seed(1)
x = matrix(rnorm(20*2), ncol = 2)
class = c(rep(-1,10),rep(1,10))
x[class == 1,] = x[class == 1,] + 1
```
Let's plot the data to see whether they are linearly seperable

```{r}
library(ggplot2)
ggplot(data = data.frame(x), aes(X1, X2, color = factor(class)))+
  geom_point()
```

The points are not linearly separable. Next, we fit the support vector classifier. Note that in order for the `svm()` function to perform **classification**, we must encode the response as a **factor**

```{r}
training_data = data.frame(x = x, class = as.factor(class))
library(e1071)

svm_fit = svm(class ~., data = training_data, 
              kernel = 'linear',
              cost = 10, 
              scale = F)
```
The argument `scale = F` tells the `svm()` function not to scale each feature to have a mean of zero and standard deviation of 1. Depending on the application, we might prefer to use `scale = TRUE`. We can plot the support vector classifier by calling `plot()` function on the output of `svm()` as well as the data used in the call of `svm()`

```{r}
plot(svm_fit, training_data)
```
The region of feature space that will be assigned to the class - 1 is shown in light yellow while the region that will be assigned to +1 is shown in red. The decision boundary between the two classes is linear (because we used the argument `kernel = 'linear'`), though due to the way in which the plotting function is implemented in this library, the decision bounday looks somewhat jagged. We seee that in this case, only few are misclassified. 

The support vector are plotted as crosses and teh remaining vectors are plotted as circles; we see here that there are 14 support vectors, we can determine their identities as follows

```{r}
names(svm_fit)
svm_fit$index
```
We can obtain some basic info about the support vector classifier fit using `summary()` command.

```{r}
summary(svm_fit)
```

This tells us, for instance, that a linaer kernel was used with `cost = 10`, and there were seven support vectors. Four in one class and three in the other. What if instead we use a smaller cost?

```{r}
svm_fit = svm(class ~ ., 
              data = training_data,
              kernel = 'linear',
              cost = .1,
              scale = F)

plot(svm_fit, training_data)
svm_fit$index
```
Now that a smaller value of `cost` parameter is being used, we obtain a larger number of support vectors because the margin is wider. Unfortunately the `svm()` function does not output the coeficients of the linear decisiion boundary obtained when support vector classifier is fit, nor does it output the width of the margin. 

the `e1071` library includes built in function `tune()` to perform cross validation. By default, `tune()` performs **10 fold cross validation** on set of model of interest. In order to use this function, we pass relevant information about set of models that are under consideration. The following command indicates that we want to compare the SVMs with linear kernel using a range of values of the cost parameter

```{r}
set.seed(1)
tune_out = tune(svm, class ~., 
                data = training_data,
                kernel = 'linear',
                ranges = list(cost = c(.001, .01, .1, 1,5,10,100)))
```

We can easily access the cross validation errors for each model using the `summary()` command:

```{r}
summary(tune_out)
```

the `tune()` function stores the best model obtained, which can be accessed as follows:

```{r}
names(tune_out)
best_mod = tune_out$best.model
summary(best_mod)
```

As usual, the `predict()` function can be used to predict the class label on a set of test obs, at any value of the cost parameter. Let's generate a test data set

```{r}
library(dplyr)
xtest = matrix(rnorm(20*2), ncol = 2)
ytest = sample(c(-1,1), 20, rep = T)
xtest[ytest ==1,] = xtest[ytest ==1, ] + 1
test_data = data.frame(xtest, class = as.factor(ytest))
test_data = test_data %>%
  rename(x.1 = X1, x.2 = X2)
```


Now we predict the class of these test obs. Here we use the best model obtained through cross validation in order to make predictions. 

```{r}
class_pred = predict(best_mod, newdata = test_data)
table(predicted = class_pred, true = test_data$class)
```


Now we consider a case where the two classes are linearly separable. Then we can find a separating hyperplane using `svm()` function. First, we'll give our simulated data a littler nudge.

```{r}
x[class ==1,] = x[class ==1,] +.5
ggplot(data.frame(x), aes(X1, X2, color = as.factor(class)))+
  geom_point()
```

Now the observations are **just barely linearly** separable. We fit support vector classifier and plot the hyperplane, using a very large value of `cost` so that no observation is misclassified

```{r}
training_data2 = data.frame(x =x, class = as.factor(class))
svm_fit = svm(class ~., data = training_data2, kernel = 'linear', cost = 1e5)
summary(svm_fit)
plot(svm_fit, training_data2)
```
No training errors were made and only three support vectors were used. However, we can see that the margin is very narrow (because the observations that are not support vectors are very close to decision boundary). it seems that this model will perform poorly on test data. Let's try a smaller value of `cost`

```{r}
svm_fit = svm(class ~., data = training_data2, kernel = 'linear', cost = 1)
summary(svm_fit)
plot(svm_fit, training_data2)
```
Using `cost = 1`, we misclassify a training obs, but we also obtain much wider margin and makes use of seven support vector. It is likely that this model will perform better.

## Support vector machine

In order to fit SVM using **non linear kernel**, we once again use `svm()` function. However, now we use a different value of the parameter kernel. To fit an `svm` with a polynomial kernel, we use `kernel = 'polynomial'` and to fit an svm with radial kernel we use `kernel = 'radial'`. In the former case, we also use the `degree` argument to specify a degree for the polynomial kernel. In the latter case we use `gamma` to specify the radial basis kernel

Let's generate some non linear class boundary

```{r}
set.seed(1)
x = matrix(rnorm(200*2), ncol = 2)
x[1:100,] = x[1:100,]+2
x[101:150,] = x[101:150, ] - 2
class = c(rep(1,150), rep(2,50))
nonlinear_data = data.frame(x = x, class = as.factor(class))

ggplot(nonlinear_data, aes(x.1, x.2, color = as.factor(class)))+
  geom_point()
```

See how one class is kind of stuck in the middle of another. This suggest that we might want to use **radial kernel** in our SVM. Now let's ready to split this training data into training and testing groups and then fit the training data into `svm()` function with a radial kernel and gamma = 1

```{r}
set.seed(1)
nonlinear_train = nonlinear_data %>%
  sample_frac(.5)

nonlinear_test = nonlinear_data %>%
  setdiff(nonlinear_train)

svm_fit = svm(class ~., data = nonlinear_train, kernel = 'radial', gamma = 1, cost = 1)
plot(svm_fit, nonlinear_train)
```
The plot shows a non linear boundary. If we increase the cost we can reduce the training errors

```{r}
svmfit = svm(class~., data = nonlinear_train, kernel = "radial", gamma = 1, cost = 1e5)
plot(svmfit, nonlinear_train)
```

This comes at a price of more irregular decision boundary that seems to be at risk of overfitting the data. We can perform cross validation using `tune()` to select the best choice of gamma and cost for svm with radial kernel

```{r}
set.seed(1)

tune_out = tune(svm, class ~., data = nonlinear_train, kernel = 'radial',
                ranges = list(cost = c(.1,1,10,100,1000), gamma = c(.5,1,2,3,4)))
bestmod = tune_out$best.model
summary(bestmod)
```

Therefore, the best choice of parameter involves `cost = 1` and `gamma = 2`. We can plot the resulting fit using plot function and predict the model to compare with the test data

```{r}
plot(bestmod, nonlinear_train)
pred = predict(bestmod, newdata = nonlinear_test)
table(true = nonlinear_test$class, pred)
```
## ROC Curves

The `ROCR` package can be used to produce ROC curves. We first write a short function to plot an ROC curve given a vector  containing numerical score for each observation, `pred` and vector containing class label for each observation `truth`

```{r}
library(ROCR)

rocplot = function(pred, truth, ...){
  predob = prediction(pred,truth)
  perf = performance(predob, 'tpr','fpr')
  plot(perf, ...)
}
```

SVMs and support vector classifier output class labels for each obs. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. For instance in the case of of a support vector classifier, the fitted value of an observation $X = (X_1,X_2, . . .,X_p)^T$ takes the form $\hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2 + . . . + \hat\beta_pX_p$.

For an SVM with a non-linear kernel, the equation that yields the fitted value is given by 9.23 on p.352 of the ISLR book. In essence, the sign of the fitted value determines on which side of the decision boundary the observartion lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero, then it is assigned to one class, if it is less than zero then it is assigned to another

In order to obtain fitted values for a given svm fit, we use `decision.values = T` when fitting svm. Then the `predict()` function will output the fitted values. Let's fit the models using $\gamma$ selected by cross validation, and a higher value, which will produce a more flexible fit

```{r}
svmfit_opt = svm(class ~., 
                 data = nonlinear_train,
                 kernel = 'radial',
                 gamma = 2, 
                 cost = 1, 
                 decision.values = T)

svmfit_flex = svm(class ~., 
                  data = nonlinear_train,
                  kernel = 'radial',
                  gamma = 50,
                  cost = 1,
                  decision.values = T)
```


```{r}
par(mfrow = c(1,2))

#plot optimal daat
fitted_opt_train = attributes(predict(svmfit_opt, nonlinear_train, decision.values = T))$decision.values
rocplot(fitted_opt_train, nonlinear_train$class, main = 'Training Data')

#add flexible data
fitted_flex_train = attributes(predict(svmfit_flex, nonlinear_train, decision.values = T))$decision.values
rocplot(fitted_flex_train, nonlinear_train$class, add = T, col = 'red')

#plot optimal model on test data
fitted_opt_test = attributes(predict(svmfit_opt, nonlinear_test, decision.values = T))$decision.values
rocplot(fitted_opt_test, nonlinear_test$class, main = 'Test Data')

#add flexible model
fitted_flex_test = attributes(predict(svmfit_flex, nonlinear_test, decision.values = T))$decision.values
rocplot(fitted_flex_test, nonlinear_test$class, add = T, col = 'red')

```

### SVM with multiple cases

If the response is a factor containing more than two levels, then the svm function will perform multi-class classification using 1v1 approach. We explore that setting here

```{r}
x = rbind(x , matrix(rnorm(50*2), ncol = 2))
class = c(class, rep(0,50))
x[class ==0,2] = x[class ==0,2]  + 2
data_3_classes =  data.frame(x =x, class = as.factor(class))

ggplot(data_3_classes, aes(x.1,x.2, color = factor(class)))+
  geom_point()
```

Fitting an SVM to multiclass use identical syntax to fitting a simple two class model

```{r}
svm_fit = svm(class ~., data = data_3_classes, kernel = 'radial', cost = 10, gamma = 1)
plot(svm_fit, data_3_classes)
```
Application to gene expression data

We know examine the `Khan` dataset, which consists of a number of tissue samples corresponding to four distinct types of small round blue tumors. For each tissue sample, gene expression measurements are available. The dataset consists of training data `xtrain` and `ytrain`, and testing data `xtest` and `ytest`


```{r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)

```

```{r}
table(Khan$ytrain)
table(Khan$ytest)
```
We will use a support vector approach to predict the cancer type subtype using gene expression measurements. In this dataset, there are a very large number of feature relative to number of observations. This suggest that we should use linear kernel, because additional flexibility that will result from polynomial or radial is unncessary

```{r}
dat = data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
out = svm(y ~., data = dat, kernel = 'linear', cost = 10)
summary(out)
```

```{r}
names(out)
table(out$fitted, dat$y)
```
We see that there are no trainign errors. In fact, this is not surprising because large number of variables relative to number of obs implies that it is easy to find a hyperplane that fully separates the classes. We are most interested not in SVMs performance on training data but rather it's performance on test obs

```{r}
dat.te = data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))
pred.te = predict(out, newdata = dat.te)
table(pred.te,dat.te$y)
```




























