```{r, message=FALSE, warning=FALSE, echo=FALSE}
require(knitr)
opts_chunk$set(eval=FALSE)
```

This lab on Linear Regression in R comes from p. 109-119 of "Introduction to Statistical Learning with Applications in R" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It was re-implemented in Fall 2016 in `tidyverse` format by Amelia McNamara and R. Jordan Crouser at Smith College.

#  3.6.1 Libraries

The `library()` function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the `MASS` package, which is a very large collection of data sets and functions. We also load the `ISLR` package, which includes the data sets associated with this book.

```{r}
library(MASS)
library(ISLR)
```

# 3.6.2 Simple Linear Regression

Unfortunately, the above line will break because `R` doesn't know where to look for the data. We'll need to tell it explicitly that `data = Boston` within this function call. Now let's fit a simple linear model (we'll call it `model_SL`):

```{r}
model_SL = lm(medv~lstat, data=Boston)
```

```{r}
names(Boston)
```

We can fit linear models using the `lm()` function. For example, we might want to fit a model with `medv` as the response and `lstat` as the predictor, which has the syntax `medv~lstat`:

```{r}
model_SL = lm(medv~lstat)
summary(model_SL)
```

```{r}
library(broom)
glance(model_SL)
augment(model_SL)
```




If we type `model_SL`, some basic information about the model is output. For more detailed information, we use `summary(model_SL)`

```{r}
model_SL
summary(model_SL)
```

We can use the `names()` function in order to find out what other pieces of information are stored in `model_SL`. Although we can extract these quantities by nameâ€”e.g. `model_SL$coefficients` â€” it is safer to use extractor functions like `coef()` to access them.

```{r}
names(model_SL)
coef(model_SL)
```

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command.

```{r}
confint(model_SL)
```

The `predict()` function can be used to produce both confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`. First, we'll make a data frame with some new values for `lstat`

```{r}
new_values = data.frame(lstat=c(5,10,15))
```

Now, we'll call the `predict()` function to see what our model predicts for the corresponding `medv` value, as well as the `confidence` or `prediction` intervals:

```{r}
predict(model_SL,new_values, interval="confidence")
predict(model_SL,new_values, interval="prediction")
```

We will now plot `medv` and `lstat` along with the least squares regression line using `ggplot()` and `geom_smooth()`:

```{r}
library(ggplot2)
ggplot(Boston, aes(x=lstat, y=medv)) +
  geom_smooth(method = "lm") +
  geom_point()
```

Below we experiment with some additional settings for plotting lines and points. The `lwd=3` command causes the width of the regression line to be increased by a factor of 3. We can also use the `shape` and `size` options to create different plotting symbols.

```{r}
ggplot(Boston, aes(x=lstat, y=medv)) +
  geom_smooth(method = "lm", lwd=3, se = FALSE) +
  geom_point(shape = "+", size = 5)
```

We can use the `ggfortify` package to produce diagnostic plots. These plots are automatically produced by applying the `autoplot()` function directly to the model.

```{r}
library(ggfortify)
autoplot(model_SL)
```

Alternatively, we can compute the residuals from a linear regression fit using the `residuals()` function. The function `rstudent()` will return the studentized residuals, and we can use this function to plot the residuals against the fitted values. To draw these plots side by side, we can use the `grid.arrange()` function from the `gridExtra` library. We'll also use the `labs()` function to add labels to our plots.

```{r}
library(gridExtra)

plot1 = ggplot() +
  geom_point(aes(predict(model_SL), residuals(model_SL))) +
  labs(x="Predicted Value", y="Residual")

plot2 = ggplot() +
  geom_point(aes(predict(model_SL), rstudent(model_SL))) +
  labs(x="Predicted Value", y="Studentized Residual")

grid.arrange(plot1, plot2, ncol=2)
```
Try it using modelr
```{r}
library(tidyverse)
library(modelr)
lm.fit = model_SL
Boston1 = Boston %>%
  select(medv, lstat)

Boston1 = Boston1 %>%
  add_residuals(lm.fit)

grid = Boston1 %>%
  data_grid(lstat)

grid = grid %>%
  add_predictions(lm.fit)

Boston1 %>%
  inner_join(grid) %>%
  ggplot(aes(pred, resid))+
  geom_point()+
  geom_smooth()
```



On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the `hatvalues()` function.

```{r}
plot(hatvalues(model_SL))
```

```{r}
augment(model_SL) %>%
  mutate(index = row_number()) %>%
  ggplot(aes(index, .hat))+
  geom_jitter()
```


The `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.

```{r}
which.max(hatvalues(model_SL))
```

#  3.6.3 Multiple Linear Regression

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(yâˆ¼x1+x2+x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors.

```{r}
model_ML = lm(medv~lstat+age, data=Boston)
summary(model_ML)
```

The Boston data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:

```{r}
model_ML = lm(medv~., data=Boston)
summary(model_ML)
```

The `vif()` function, part of the `car` package, can be used to compute variance inflation factors. Most VIFs are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the `install.packages` option in R.

```{r}
library(car)
vif(model_ML)
```

What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, `age` has a high p-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except `age`.

```{r}
model_ML_no_age = lm(medv~.-age, data=Boston)
summary(model_ML_no_age)
```

Alternatively, we can use the `update()` function to return an updated version of our previous `model_ML`.

```{r}
model_ML_no_age = update(model_ML, ~.-age)
```

# 3.6.4 Interaction Terms

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells R to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age`, and the interaction term `lstatÃ—age` as predictors; it is a shorthand for `lstat+age+lstat:age`.

```{r}
summary(lm(medv~lstat*age, data=Boston))
```

# 3.6.5 Non-linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor `X`, we can create a predictor `X2` using `I(X^2)`. The function `I()` is needed since the ^ has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise `X` to the power 2. We now perform a regression of `medv` onto `lstat` and `lstat2`.

```{r}
model_ML_quadratic=lm(medv~lstat+I(lstat^2), data=Boston)
summary(model_ML_quadratic)
```
```{r}
summary(model_SL)
```

The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{r}
anova(model_SL, model_ML_quadratic)
```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadraticmodel that has two predictors, `lstat` and `lstat2`. The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. 

The F-statistic is 135, and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. 

If we type:

```{r}
autoplot(model_ML_quadratic)
```
compare it to model_sl

```{r}
autoplot(model_SL)
```


then we see that when the `lstat2` term is included in the model, there is little discernible pattern in the residuals.

In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a fifth-order polynomial fit:

```{r}
model_ML_5th_order_poly = lm(medv~poly(lstat, 5, raw=TRUE), data=Boston)
summary(model_ML_5th_order_poly)
```

This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant p-values in a regression fit.

Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.

```{r}
summary(lm(medv~log(rm), data=Boston))
```

# 3.6.6 Qualitative Predictors

We will now examine the `Carseats` data, which is part of the `ISLR` library. We will attempt to predict `Sales` (child car seat sales) in 400 locations based on a number of predictors.

```{r}
Carseats %>% view()
str(Carseats)
names(Carseats)
```

The `Carseats` data includes qualitative predictors such as `Shelveloc`, an indicator of the quality of the shelving locationâ€”that is, the space within a store in which the car seat is displayedâ€”at each location. The predictor `Shelveloc` takes on three possible values, `Bad`, `Medium`, and `Good`.

Given a qualitative variable such as `Shelveloc`, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.

```{r}
lm.fit=lm(Sales~.+Income:Advertising+Price:Age, data=Carseats)
summary(lm.fit)
```

The `contrasts()` function returns the coding that R uses for the dummy variables. Use `?contrasts` to learn about other contrasts, and how to set them.

```{r}
contrasts(Carseats$ShelveLoc)
```


### Exercises

create a simple linear regression using mpg as the response to horsepower
```{r}
auto = Auto
str(auto)

model1 = lm(mpg~horsepower, data = auto)
summary(model1)
```
```{r}
auto %>%
  ggplot(aes(mpg, horsepower))+
  geom_point()
```
```{r}
autoplot(model1)
```
The model is not good. Horsepower is significant to mpg but based from the plots, it could be transformed to find a better fit.

1. Is there a relationship? - Yes
2. How strong is the relation - strong relationship
3. negative relationship. SRE is 5 so we could be off by 5 mpg
4. what is the predicted mpg with a horsepower of 98

Overpredicted

```{r}
library(modelr)
auto %>%
  select(horsepower, mpg) %>%
  add_predictions(model1) %>%
  filter(horsepower==98)
```

Add a fitted line

```{r}
coef(model1)[1]
auto %>%
  ggplot(aes(mpg, horsepower))+
  geom_point()+
  geom_smooth()
```

#. Multiple Linar Regression for auto dataset

Produce a scatterplot matrix of all variables

```{r}
auto = auto %>%
  mutate(origin = factor(origin))
pairs(auto)
```
Corplot for quantitative variables

```{r}
library(corrplot)
auto_corr = Auto %>% 
  select(-name)

str(auto_corr)
cor(auto_corr)
```

```{r}
lm_ml = lm(mpg ~., data = auto %>% select(-name))
summary(lm_ml)
tidy(lm_ml)
```
1. Is there a relationship between predictors and response. Yes. the pvalue is low for fstatic.
2. which predictors have statistically significant relationship: displacement, weight, year, origin
3. what does the coefficient of year suggest: higher year, faster mpg

```{r}
auto %>%
  select(mpg, year) %>%
  group_by(year) %>%
  mutate(mpg = mean(mpg)) %>%
  ggplot(aes(year, mpg))+
  geom_line(aes(group = 1))
```

D. use plot to create diagnostic plots of the fit

```{r}
Auto1= Auto %>%
  select(-name) %>%
  mutate(origin = factor(origin))

auto %>%
  add_predictions(lm_ml) %>%
  add_residuals(lm_ml) %>%
  ggplot(aes(pred, resid))+
  geom_point()+
  geom_smooth()
```


```{r}
autoplot(lm_ml)
```

FIt interaction effects

```{r}
lm_ml2 = lm(mpg~. + year:weight + year:horsepower + year*origin, data = Auto1)
summary(lm_ml2)
```
Year and Horsepower

year and horsepower is significant

Try a few transformation

```{r}
lm_ml3 = lm(log(mpg) ~., data = Auto1)
summary(lm_ml3)

autoplot(lm_ml3)
```
log transformation on mpg produced a more accurate model.


# Carseats dataset

Fir a multiple regression to predict sales using price, urban, and US
```{r}
lm_ml4 = lm(Sales ~ Price + Urban + US, data = Carseats)
summary(lm_ml4)
```
Can reject null hypothesis for US and Price


```{r}
autoplot(lm_ml4)
```

Fit only models where there is association

```{r}
lm_ml5 = lm(Sales ~ Price + US, data = Carseats)
summary(lm_ml5)
```
Create a 95 percent interval

```{r}
confint(lm_ml5)
```

Is there evidence of outliers or high leverage

```{r}
autoplot(lm_ml5)
```

```{r}
augment(lm_ml5) %>%
  mutate(index = row_number()) %>%
  filter(.hat >= 3/400)

```


```{r}
augment(lm_ml5) %>%
  filter(abs(.std.resid) > 3)
```
#11
In this problem we will investigate t-statistic for null hypothesis Ho: B= 0 in simple linear regression without an intercept. 

```{r}
set.seed(1)
x = rnorm(100)
y = 2*x + rnorm(100)
```


Perform a simple linear regression of y onto x without an intercept. 

```{r}
lm1 = lm(y~x+0)
summary(lm1)
```

```{r}
confint(lm1)
```


Now perform x onto y

```{r}
lm2 = lm(x~y+0)
summary(lm2)
```
Same t-statistics and R-squared. 

```{r}
set.seed(1)
x = rnorm(100)
eps = rnorm(100, mean = 0, sd = .25)
y = -1 + .5*x + eps

plot(x,y)
```

Fit a least squares line

```{r}
lm3 = lm(y~x)
summary(lm3)
```

Plot both lines

```{r}
plot(x,y);abline(-1,.5);abline(coef(lm3),col="red")
plot(x,y)
abline(lm3, col = 'red')
abline(-1,.5)
```

Fit a polynomial for x and x2

```{r}
lm4 = lm(y ~ x + I(x^2))
summary(lm4)
```

Higher R squared but higher residual error also. Also the component is not significant

Repeat the steps for more noise. increase the sd

```{r}
set.seed(1)
x = rnorm(100)
eps = rnorm(100, mean = 0, sd = .5)
y2 = -1 + .5*x + eps

plot(x,y2)
```
```{r}
lm5 = lm(y2~x)
summary(lm5)
```

```{r}
plot(x,y);abline(-1,.5);abline(coef(lm5),col="red")
```
The fitted line did not change much, but it could be observed that RSE and R^2, the parameters to assess the validity of the model became worse. 

# 14. This problem is on collinearity


```{r}
set.seed(1)
x1=runif(100)
x2=0.5*x1+rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
```


Scatterplot between variables
```{r}
data = data.frame(
  x1 = x1,
  x2 = x2,
  y = y
)

pairs(data)
corrplot::corrplot(cor(data), method = 'shade', tl.col = 'black', addCoef.col = 'black')
```
X1 and X2 are highly correlated


```{r}
lm_cor = lm(y~x1+x2, data = data)
summary(lm_cor)
```
Regression coefficients are far from the real value. Low f statistic. Not significant coefficients. Intercept is close and we can not reject. But B1 and B2 are subject for rejection. 

```{r}
autoplot(lm_cor)
```


```{r}
library(car)
vif(lm_cor)
```
Fit only using x1

```{r}
lm_x1 = lm(y~x1, data = data)
summary(lm_x1)
```
```{r}
lm_x2 = lm(y~x2, data = data)
summary(lm_x2)
```
The hypothesis for null has become stronger. the estimate came closer to the true value too. This example only shows the effect of collinearity in regression

#Boston Dataset

Predict capita crime rate using other variables in the data.

```{r}
str(Boston)
Boston_fct = Boston %>%
  mutate(chas = factor(chas),
         rad = factor(rad))
```


```{r}
pairs(Boston)
```

```{r}
corrplot(cor(Boston), method = 'shade', tl.col = 'black', addCoef.col = 'black')
```

```{r}
lm_boston = lm(crim ~ ., data = Boston)
summary(lm_boston)
tidy(lm_boston) %>%
  filter(p.value < .005)
```
Which predictors can we 


```{r}
preds = names(Boston)[-1]


data = data.frame(NULL)

for (i in preds){
  model = tidy(lm(as.formula(paste0('crim~',i)), data = Boston))
  data = rbind(data, model)
}

individual = data %>%
  filter(term != ('(Intercept)'))

group = tidy(lm_boston)

summarize = individual %>%
  select(term, estimate) %>%
  inner_join(group %>% select(term, estimate), by = 'term')

names = c('term','individual_reg', 'group_reg')
names(summarize) = names
summarize %>%
  gather(key = 'type', value = 'value', -term) %>%
  ggplot(aes(term, value, color = type))+
  geom_point()
```

```{r}
individual %>%
  filter(p.value < .05)
```


```{r}
summarize
```

The individual regression gave much larger coefficients








