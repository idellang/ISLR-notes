---
title: "Chap10 - Exercises"
author: "Me"
date: "8/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#Q7

In the chapter, we mentioned the use of correlation based distance and Euclidian distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to zero and standard deviation of one, and if we let $rij$ denote the correlation between $ith$ and $jth$ obs, then the quantity 1-$rij$ is proportional to squared Euclidian distance between the ith and jth obs.

On the USArrest data, show that this proportionality holds


```{r}
library(ISLR)
set.seed(1)
dsc = scale(USArrests)
d1 = dist(dsc)^2
d2 = as.dist(1 - cor(t(dsc)))
```


#Q8 
In the section 10.2.3, a formula for calculating PVE was given in Equation 10.8. We also saw that the PVE can be obtained using sdev output of the `prcomp()` function. 

On the USArrest data, calculate PVE in two ways:

1. using sdev
2. By applying equation 10.8 directly. That is, use `prcomp()` function to compute principal component loadings. Then, use those loadings.

```{r}
pr.out = prcomp(USArrests, scale = T)
pr.var = pr.out$sdev^2
pve = pr.var/sum(pr.var)
sum(pr.var)
```


```{r}
loadings = pr.out$rotation
USArrests2 = scale(USArrests)
sum_var = apply(as.matrix(USArrests2)^2, 2, sum)

apply((as.matrix(USArrests2) %*% loadings)^2, 2, sum) / sum_var
```

#Q9.

Consider the `USArrest` data. We will now perform hierarchical clustering on the states

using hierarchical clustering with complete linkage and euclidian distance, cluster the states

```{r}
library(ggdendro)
set.seed(2)
hc.complete = hclust(dist(USArrests), method = 'complete')
plot(hc.complete)
ggdendrogram(hc.complete, size = 2)
```

Cut the dendogram at the height that results to 3 distinct clusters. which states belong to which clusters

```{r}
library(dplyr)
library(tidyr)
cutree(hc.complete,3)
  
```

Hierarchically cluster the states using linkage and Euclidian distance after scaling.

```{r}
scaled_data = scale(USArrests)
hc.complete.sd = hclust(dist(scaled_data), method = 'complete')
ggdendrogram(hc.complete.sd)
```
What effect does scaling the variable have on hierarchical clsutering obtained. In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed?

```{r}
cutree(hc.complete.sd, 3)
```

```{r}
table(cutree(hc.complete,3), cutree(hc.complete.sd,3))
```

Scaling the variable affect the clusters obtained although the trees are somewhat similar. The variables should be scaled before hand because the data measures have different units


#Q10

In this problem, you will generate simulated data and then perform PCA and K-means clustering on the data

Generate a simulated data set with 20 obs in each three classes and 50 variables

```{r}
set.seed(2)
x  = matrix(rnorm(20*3*50, mean = 0 , sd = .001), ncol = 50)
x[1:20, 2] = 1
x[21:40, 1] = 2
x[41:60, 1] =1

true.labels = c(rep(1,20), rep(2,20), rep(3,20))

```

Perform PCa on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the obs of each classes. If the three classes appear separated on the plot, then continue. If not, modify the simulation so that there is greater separation on 3 classes

```{r}
library(ggplot2)
pr.out = prcomp(x)
plot(pr.out$x[,1:2], col = 1:3, xlab = 'Z1', ylab = 'Z2', pch = 19)
```

Perform K-means clustering with K=3. How well do clusters you obtained in K means compare to the true label

```{r}
km.out = kmeans(x, 3, nstart = 20)
table(true.labels, km.out$cluster)
```
Perfectly clustered

Perform K = 2 and describe your results

```{r}
km.out = kmeans(x, 2, nstart = 20)
table(true.labels, km.out$cluster)
```

All observations of one of the three clusters is now absorbed by 1

Perform k = 4
```{r}
km.out = kmeans(x, 4 , nstart = 20)
table(true.labels, km.out$cluster)
```

Now perform K-means clustering with K = 3 on the first two principal components score vectors rather than the raw data. That is, perform k means on the 60 x 2 matrix which the first column is the first principal component score vector, and the second column is the principal component score vector

```{r}
km.out = kmeans(pr.out$x[,1:2], 3, nstart = 20)
table(true.labels, km.out$cluster)
```
Perfectly clustered once again.

Using the scale function, perform K means with K = 3 after scaling each data to have standard deviation of one. 

```{r}
km.out = kmeans(scale(x), 3, ,nstart = 20)
table(true.labels, km.out$cluster)
```
Worst results than unscaled data, as scaling affects the distance between obs


#Q11

Load the data using read.csv()

```{r}
genes = read.csv('Ch10Ex11.csv', header = F)
```


Apply hierarchical clustering to the samples using correlation based distance, and plot the dendogram. Do the genes separate the samples into to groups? Do your result depend on the type of linkage

```{r}
hc.complete = hclust(as.dist(1 - cor(genes)), method = 'complete')
ggdendrogram(hc.complete)
```

Using sing;e

```{r}
hc.single = hclust(as.dist(1 - cor(genes)), method = 'single')
ggdendrogram(hc.single)
```

Using average

```{r}
hc.average = hclust(as.dist(1 - cor(genes)), method = 'average')
ggdendrogram(hc.average)
```

Your collaborator wants to know which genes differ the most. Suggest a way to answer this question

```{r}
pr.out = prcomp(t(genes))
pr.out$rotation %>%
  as.data.frame()
```


```{r}
total.load = apply(pr.out$rotation, 1, sum)
index = order(abs(total.load), decreasing = T)
index[1:10]
```




